!_TAG_FILE_FORMAT	2	/extended format; --format=1 will not append ;" to lines/
!_TAG_FILE_SORTED	1	/0=unsorted, 1=sorted, 2=foldcase/
!_TAG_PROGRAM_AUTHOR	Darren Hiebert	/dhiebert@users.sourceforge.net/
!_TAG_PROGRAM_NAME	Exuberant Ctags	//
!_TAG_PROGRAM_URL	http://ctags.sourceforge.net	/official site/
!_TAG_PROGRAM_VERSION	5.9~svn20110310	//
ATA	prune/rescale.py	/^    ATA = torch.zeros(num_neurons, num_neurons)# .cuda()$/;"	v
ATB	prune/rescale.py	/^    ATB = torch.zeros(num_neurons)#.cuda()$/;"	v
AverageMeter	utils/meter.py	/^class AverageMeter:$/;"	c
B	prune/rescale.py	/^        B = teacher_output - ffn2.fc1.bias - input_tensor #dense.bias - input_tensor$/;"	v
BertFFN	generate_lut.py	/^class BertFFN(nn.Module):$/;"	c
BertMHA	generate_lut.py	/^class BertMHA(nn.Module):$/;"	c
CAPS_PATH	dataset/MSCOCO.py	/^    "annotations" \/ "captions_val2017.json"$/;"	v
CLIPAttention	modeling_clip.py	/^class CLIPAttention(nn.Module):$/;"	c
CLIPConfig	configuration_clip.py	/^class CLIPConfig(PretrainedConfig):$/;"	c
CLIPEncoder	modeling_clip.py	/^class CLIPEncoder(nn.Module):$/;"	c
CLIPEncoderLayer	modeling_clip.py	/^class CLIPEncoderLayer(nn.Module):$/;"	c
CLIPMLP	modeling_clip.py	/^class CLIPMLP(nn.Module):$/;"	c
CLIPModel	modeling_clip.py	/^class CLIPModel(CLIPPreTrainedModel):$/;"	c
CLIPOnnxConfig	configuration_clip.py	/^class CLIPOnnxConfig(OnnxConfig):$/;"	c
CLIPOutput	modeling_clip.py	/^class CLIPOutput(ModelOutput):$/;"	c
CLIPPreTrainedModel	modeling_clip.py	/^class CLIPPreTrainedModel(PreTrainedModel):$/;"	c
CLIPTextConfig	configuration_clip.py	/^class CLIPTextConfig(PretrainedConfig):$/;"	c
CLIPTextEmbeddings	modeling_clip.py	/^class CLIPTextEmbeddings(nn.Module):$/;"	c
CLIPTextModel	modeling_clip.py	/^class CLIPTextModel(CLIPPreTrainedModel):$/;"	c
CLIPTextTransformer	modeling_clip.py	/^class CLIPTextTransformer(nn.Module):$/;"	c
CLIPVisionConfig	configuration_clip.py	/^class CLIPVisionConfig(PretrainedConfig):$/;"	c
CLIPVisionEmbeddings	modeling_clip.py	/^class CLIPVisionEmbeddings(nn.Module):$/;"	c
CLIPVisionModel	modeling_clip.py	/^class CLIPVisionModel(CLIPPreTrainedModel):$/;"	c
CLIPVisionTransformer	modeling_clip.py	/^class CLIPVisionTransformer(nn.Module):$/;"	c
CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP	configuration_clip.py	/^CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP = {$/;"	v
CLIP_PRETRAINED_MODEL_ARCHIVE_LIST	modeling_clip.py	/^CLIP_PRETRAINED_MODEL_ARCHIVE_LIST = [$/;"	v
CPUTimer	utils/timer.py	/^class CPUTimer:$/;"	c
GLUE_TASKS	dataset/glue.py	/^GLUE_TASKS = [$/;"	v
GPUTimer	utils/timer.py	/^class GPUTimer:$/;"	c
IMGS_DIR	dataset/MSCOCO.py	/^IMGS_DIR = pathlib.Path(__file__).parent.resolve() \/ "val2017"$/;"	v
MSCOCO	dataset/MSCOCO.py	/^class MSCOCO(Dataset):$/;"	c
MaskNeurons	utils/arch.py	/^class MaskNeurons:$/;"	c
PiecewiseLinearLatency	efficiency/latency.py	/^class PiecewiseLinearLatency:$/;"	c
W	prune/rescale.py	/^    W = weights_per_neuron @ weights_per_neuron.t()$/;"	v
_CHECKPOINT_FOR_DOC	modeling_clip.py	/^_CHECKPOINT_FOR_DOC = "openai\/clip-vit-base-patch32"$/;"	v
__enter__	utils/arch.py	/^    def __enter__(self):$/;"	m	class:MaskNeurons	file:
__enter__	utils/timer.py	/^    def __enter__(self):$/;"	m	class:CPUTimer	file:
__enter__	utils/timer.py	/^    def __enter__(self):$/;"	m	class:GPUTimer	file:
__exit__	utils/arch.py	/^    def __exit__(self, type, value, traceback):$/;"	m	class:MaskNeurons	file:
__exit__	utils/timer.py	/^    def __exit__(self, type, value, traceback):$/;"	m	class:CPUTimer	file:
__exit__	utils/timer.py	/^    def __exit__(self, type, value, traceback):$/;"	m	class:GPUTimer	file:
__getitem__	dataset/MSCOCO.py	/^    def __getitem__(self, index):$/;"	m	class:MSCOCO	file:
__init__	configuration_clip.py	/^    def __init__($/;"	m	class:CLIPConfig
__init__	configuration_clip.py	/^    def __init__($/;"	m	class:CLIPTextConfig
__init__	configuration_clip.py	/^    def __init__($/;"	m	class:CLIPVisionConfig
__init__	dataset/MSCOCO.py	/^    def __init__(self, n_samples, name):$/;"	m	class:MSCOCO
__init__	efficiency/latency.py	/^    def __init__(self, threshold=None, c=None, slope=None):$/;"	m	class:PiecewiseLinearLatency
__init__	generate_lut.py	/^    def __init__($/;"	m	class:BertMHA
__init__	generate_lut.py	/^    def __init__(self, hidden_size, intermediate_size):$/;"	m	class:BertFFN
__init__	modeling_clip.py	/^    def __init__(self, config):$/;"	m	class:CLIPAttention
__init__	modeling_clip.py	/^    def __init__(self, config):$/;"	m	class:CLIPMLP
__init__	modeling_clip.py	/^    def __init__(self, config: CLIPConfig):$/;"	m	class:CLIPEncoder
__init__	modeling_clip.py	/^    def __init__(self, config: CLIPConfig):$/;"	m	class:CLIPEncoderLayer
__init__	modeling_clip.py	/^    def __init__(self, config: CLIPConfig):$/;"	m	class:CLIPModel
__init__	modeling_clip.py	/^    def __init__(self, config: CLIPTextConfig):$/;"	m	class:CLIPTextEmbeddings
__init__	modeling_clip.py	/^    def __init__(self, config: CLIPTextConfig):$/;"	m	class:CLIPTextModel
__init__	modeling_clip.py	/^    def __init__(self, config: CLIPTextConfig):$/;"	m	class:CLIPTextTransformer
__init__	modeling_clip.py	/^    def __init__(self, config: CLIPVisionConfig):$/;"	m	class:CLIPVisionEmbeddings
__init__	modeling_clip.py	/^    def __init__(self, config: CLIPVisionConfig):$/;"	m	class:CLIPVisionModel
__init__	modeling_clip.py	/^    def __init__(self, config: CLIPVisionConfig):$/;"	m	class:CLIPVisionTransformer
__init__	utils/arch.py	/^    def __init__(self, model, neuron_mask):$/;"	m	class:MaskNeurons
__init__	utils/meter.py	/^     def __init__(self, name):$/;"	m	class:AverageMeter
__init__	utils/timer.py	/^    def __init__(self, timelogs):$/;"	m	class:CPUTimer
__init__	utils/timer.py	/^    def __init__(self, timelogs):$/;"	m	class:GPUTimer
__len__	dataset/MSCOCO.py	/^    def __len__(self):$/;"	m	class:MSCOCO	file:
__str__	utils/meter.py	/^     def __str__(self):$/;"	m	class:AverageMeter	file:
_build_causal_attention_mask	modeling_clip.py	/^    def _build_causal_attention_mask(self, bsz, seq_len, dtype):$/;"	m	class:CLIPTextTransformer
_expand_mask	modeling_clip.py	/^def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):$/;"	f
_get_image	dataset/MSCOCO.py	/^    def _get_image(self, image_id):$/;"	m	class:MSCOCO
_init_weights	modeling_clip.py	/^    def _init_weights(self, module):$/;"	m	class:CLIPPreTrainedModel
_keys_to_ignore_on_load_missing	modeling_clip.py	/^    _keys_to_ignore_on_load_missing = [r"position_ids"]$/;"	v	class:CLIPPreTrainedModel
_no_split_modules	modeling_clip.py	/^    _no_split_modules = ["CLIPEncoderLayer"]$/;"	v	class:CLIPTextModel
_set_gradient_checkpointing	modeling_clip.py	/^    def _set_gradient_checkpointing(self, module, value=False):$/;"	m	class:CLIPPreTrainedModel
_shape	modeling_clip.py	/^    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):$/;"	m	class:CLIPAttention
apply_neuron_mask	utils/arch.py	/^def apply_neuron_mask(model, neuron_mask):$/;"	f
args	generate_lut.py	/^    args = parser.parse_args()$/;"	v
atol_for_validation	configuration_clip.py	/^    def atol_for_validation(self) -> float:$/;"	m	class:CLIPOnnxConfig
attention_mask	prune/rescale.py	/^        attention_mask = (teacher_batch[1] == 0)$/;"	v
avg_seq_length	dataset/glue.py	/^def avg_seq_length(task_name):$/;"	f
base_model_prefix	modeling_clip.py	/^    base_model_prefix = "clip"$/;"	v	class:CLIPPreTrainedModel
clip_loss	modeling_clip.py	/^def clip_loss(similarity: torch.Tensor) -> torch.Tensor:$/;"	f
closed_form_solver	utils/linalg.py	/^def closed_form_solver(A, B):$/;"	f
cls_only	prune/rescale.py	/^                cls_only=cls_only,$/;"	v
cls_only	prune/rescale.py	/^            cls_only = classification_task and (layer_idx == num_hidden_layers - 1)$/;"	v
collect_layer_inputs	utils/arch.py	/^def collect_layer_inputs($/;"	f
collect_mask_grads	prune/fisher.py	/^def collect_mask_grads(model, head_mask, neuron_mask, dataloader):$/;"	f
compute_fisher_info	prune/fisher.py	/^def compute_fisher_info(grads):$/;"	f
compute_mac	efficiency/mac.py	/^def compute_mac($/;"	f
compute_mask_mac	efficiency/mac.py	/^def compute_mask_mac(head_mask, neuron_mask, seq_len, hidden_size):$/;"	f
config	generate_lut.py	/^    config = AutoConfig.from_pretrained(args.model_name)$/;"	v
config_class	modeling_clip.py	/^    config_class = CLIPConfig$/;"	v	class:CLIPModel
config_class	modeling_clip.py	/^    config_class = CLIPConfig$/;"	v	class:CLIPPreTrainedModel
config_class	modeling_clip.py	/^    config_class = CLIPTextConfig$/;"	v	class:CLIPTextModel
config_class	modeling_clip.py	/^    config_class = CLIPVisionConfig$/;"	v	class:CLIPVisionModel
contrastive_loss	modeling_clip.py	/^def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:$/;"	f
create_and_fill_np_array	dataset/squad.py	/^def create_and_fill_np_array(start_or_end_logits, dataset, max_len):$/;"	f
create_custom_forward	modeling_clip.py	/^                def create_custom_forward(module):$/;"	f	function:CLIPEncoder.forward
custom_forward	modeling_clip.py	/^                    def custom_forward(*inputs):$/;"	f	function:CLIPEncoder.forward.create_custom_forward
default_onnx_opset	configuration_clip.py	/^    def default_onnx_opset(self) -> int:$/;"	m	class:CLIPOnnxConfig
estimate_latency	efficiency/latency.py	/^def estimate_latency(mha_lut, ffn_lut, head_mask, neuron_mask):$/;"	f
eval_glue_acc	evaluate/glue.py	/^def eval_glue_acc(model, head_mask, neuron_mask, dataloader, task_name):$/;"	f
eval_squad_acc	evaluate/squad.py	/^def eval_squad_acc($/;"	f
eval_squad_loss	evaluate/squad.py	/^def eval_squad_loss($/;"	f
ffn2	prune/rescale.py	/^    ffn2 = get_ffn2(model, layer_idx)$/;"	v
ffn_latencies	generate_lut.py	/^    ffn_latencies = ffn_lut(config, args.device, input_shape)$/;"	v
ffn_lut	generate_lut.py	/^def ffn_lut(config, device, input_shape, num_warmup=10, num_iter=10):$/;"	f
fit_latency_fn	efficiency/latency.py	/^def fit_latency_fn(lut):$/;"	f
forward	generate_lut.py	/^    def forward(self, hidden_states):$/;"	m	class:BertFFN
forward	generate_lut.py	/^    def forward(self, hidden_states):$/;"	m	class:BertMHA
forward	modeling_clip.py	/^    def forward($/;"	m	class:CLIPAttention
forward	modeling_clip.py	/^    def forward($/;"	m	class:CLIPEncoder
forward	modeling_clip.py	/^    def forward($/;"	m	class:CLIPEncoderLayer
forward	modeling_clip.py	/^    def forward($/;"	m	class:CLIPModel
forward	modeling_clip.py	/^    def forward($/;"	m	class:CLIPTextEmbeddings
forward	modeling_clip.py	/^    def forward($/;"	m	class:CLIPTextModel
forward	modeling_clip.py	/^    def forward($/;"	m	class:CLIPTextTransformer
forward	modeling_clip.py	/^    def forward($/;"	m	class:CLIPVisionModel
forward	modeling_clip.py	/^    def forward($/;"	m	class:CLIPVisionTransformer
forward	modeling_clip.py	/^    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:$/;"	m	class:CLIPMLP
forward	modeling_clip.py	/^    def forward(self, pixel_values: torch.FloatTensor) -> torch.Tensor:$/;"	m	class:CLIPVisionEmbeddings
from_pretrained	configuration_clip.py	/^    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> "PretrainedConfig":$/;"	m	class:CLIPTextConfig
from_pretrained	configuration_clip.py	/^    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> "PretrainedConfig":$/;"	m	class:CLIPVisionConfig
from_text_vision_configs	configuration_clip.py	/^    def from_text_vision_configs(cls, text_config: CLIPTextConfig, vision_config: CLIPVisionConfig, **kwargs):$/;"	m	class:CLIPConfig
generate_dummy_inputs	configuration_clip.py	/^    def generate_dummy_inputs($/;"	m	class:CLIPOnnxConfig
get_backbone	utils/arch.py	/^def get_backbone(model):$/;"	f
get_classifier	utils/arch.py	/^def get_classifier(model):$/;"	f
get_encoder	utils/arch.py	/^def get_encoder(model):$/;"	f
get_ffn1	utils/arch.py	/^def get_ffn1(model, index):$/;"	f
get_ffn2	utils/arch.py	/^def get_ffn2(model, index):$/;"	f
get_ffn_lstsq	prune/rescale.py	/^def get_ffn_lstsq($/;"	f
get_image_features	modeling_clip.py	/^    def get_image_features($/;"	m	class:CLIPModel
get_input_embeddings	modeling_clip.py	/^    def get_input_embeddings(self) -> nn.Module:$/;"	m	class:CLIPTextModel
get_input_embeddings	modeling_clip.py	/^    def get_input_embeddings(self) -> nn.Module:$/;"	m	class:CLIPVisionModel
get_layers	utils/arch.py	/^def get_layers(model):$/;"	f
get_mha_lstsq	prune/rescale.py	/^def get_mha_lstsq($/;"	f
get_mha_proj	utils/arch.py	/^def get_mha_proj(model, index):$/;"	f
get_pruning_schedule	utils/schedule.py	/^def get_pruning_schedule(target, num_iter):$/;"	f
get_text_features	modeling_clip.py	/^    def get_text_features($/;"	m	class:CLIPModel
glue_dataloader	dataset/glue.py	/^def glue_dataloader(task_name, tokenizer, training, batch_size=32, max_seq_len=None, pad_to_max=False):$/;"	f
glue_dataset	dataset/glue.py	/^def glue_dataset(task_name, tokenizer, training, max_seq_len, pad_to_max=False):$/;"	f
greedy_rearrange	prune/rearrange.py	/^def greedy_rearrange(mask, grads):$/;"	f
handle	prune/rescale.py	/^    handle = hijack_input(ffn2, inputs)$/;"	v
help	main.py	/^    help="MAC\/latency constraint relative to the original model",$/;"	v
hidden_states	prune/rescale.py	/^            hidden_states = hidden_states[:, 0, :]$/;"	v
hidden_states	prune/rescale.py	/^            hidden_states = remove_padding(hidden_states, attention_mask)$/;"	v
hidden_states	prune/rescale.py	/^        hidden_states = hidden_states.index_select(dim=0, index=nonzero_neurons)$/;"	v
hidden_states	prune/rescale.py	/^        hidden_states = hidden_states.t()$/;"	v
hijack_input	utils/arch.py	/^def hijack_input(module, list_to_append):$/;"	f
input_shape	generate_lut.py	/^    input_shape = (args.bs, args.seq_len, config.hidden_size)$/;"	v
input_tensor	prune/rescale.py	/^            input_tensor = input_tensor[:, 0, :]$/;"	v
input_tensor	prune/rescale.py	/^            input_tensor = remove_padding(input_tensor, attention_mask)$/;"	v
inputs	configuration_clip.py	/^    def inputs(self) -> Mapping[str, Mapping[int, str]]:$/;"	m	class:CLIPOnnxConfig
inputs	prune/rescale.py	/^    inputs = []$/;"	v
is_composition	configuration_clip.py	/^    is_composition = True$/;"	v	class:CLIPConfig
layer	prune/rescale.py	/^    layer = get_layers(model)[layer_idx]$/;"	v
logger	configuration_clip.py	/^logger = logging.get_logger(__name__)$/;"	v
logger	main.py	/^logger = logging.getLogger(__name__)$/;"	v
logger	modeling_clip.py	/^logger = logging.get_logger(__name__)$/;"	v
lookup_latency	efficiency/latency.py	/^def lookup_latency(lut, mask):$/;"	f
lsmr_cupy_solver	utils/linalg.py	/^def lsmr_cupy_solver(A, B):$/;"	f
mac_per_head	efficiency/mac.py	/^def mac_per_head($/;"	f
mac_per_neuron	efficiency/mac.py	/^def mac_per_neuron(seq_len, hidden_size):$/;"	f
main	main.py	/^def main():$/;"	f
main_input_name	modeling_clip.py	/^    main_input_name = "pixel_values"$/;"	v	class:CLIPVisionModel
max_seq_length	dataset/glue.py	/^def max_seq_length(task_name):$/;"	f
mha_latencies	generate_lut.py	/^    mha_latencies = mha_lut(config, args.device, input_shape)$/;"	v
mha_lut	generate_lut.py	/^def mha_lut(config, device, input_shape, num_warmup=100, num_iter=100):$/;"	f
model	prune_clip.py	/^model = CLIPModel.from_pretrained('openai\/clip-vit-base-patch32')$/;"	v
model_type	configuration_clip.py	/^    model_type = "clip"$/;"	v	class:CLIPConfig
model_type	configuration_clip.py	/^    model_type = "clip_text_model"$/;"	v	class:CLIPTextConfig
model_type	configuration_clip.py	/^    model_type = "clip_vision_model"$/;"	v	class:CLIPVisionConfig
nonzero_heads	prune/rescale.py	/^            nonzero_heads = rescaled_head_mask[layer_idx].nonzero().flatten()$/;"	v
nonzero_neurons	prune/rescale.py	/^            nonzero_neurons = rescaled_neuron_mask[layer_idx].nonzero().flatten()$/;"	v
nonzero_neurons	prune/rescale.py	/^    nonzero_neurons = student_neuron_mask[layer_idx].nonzero().flatten()$/;"	v
num_hidden_layers	prune/rescale.py	/^    num_hidden_layers = config.num_hidden_layers$/;"	v
num_labels	dataset/glue.py	/^def num_labels(task_name):$/;"	f
num_neurons	prune/rescale.py	/^    num_neurons = nonzero_neurons.shape[0]$/;"	v
outputs	configuration_clip.py	/^    def outputs(self) -> Mapping[str, Mapping[int, str]]:$/;"	m	class:CLIPOnnxConfig
parser	generate_lut.py	/^    parser = argparse.ArgumentParser()$/;"	v
parser	main.py	/^parser = argparse.ArgumentParser()$/;"	v
post_processing_function	dataset/squad.py	/^def post_processing_function(task_name, examples, dataset, predictions, stage="eval"):$/;"	f
postprocess_qa_predictions	dataset/squad.py	/^def postprocess_qa_predictions($/;"	f
prepare_train_features	dataset/squad.py	/^def prepare_train_features($/;"	f
prepare_validation_features	dataset/squad.py	/^def prepare_validation_features($/;"	f
preprocess_glue	dataset/glue.py	/^def preprocess_glue(examples, tokenizer, sentence_keys, max_seq_len, pad_to_max, label_key="label"):$/;"	f
rearrange_mask	prune/rearrange.py	/^def rearrange_mask(mask, grads):$/;"	f
register_mask	utils/arch.py	/^def register_mask(module, mask):$/;"	f
remove_padding	utils/arch.py	/^def remove_padding(hidden_states, attention_mask):$/;"	f
rescale_mask	prune/rescale.py	/^def rescale_mask($/;"	f
rescaled_head_mask	prune/rescale.py	/^    rescaled_head_mask = student_head_mask.clone()$/;"	v
rescaled_neuron_mask	prune/rescale.py	/^    rescaled_neuron_mask = student_neuron_mask.clone()$/;"	v
reset	utils/meter.py	/^     def reset(self):$/;"	m	class:AverageMeter
scale_factor	prune/rescale.py	/^            scale_factor = scale_factor[:-1]$/;"	v
search_latency	prune/search.py	/^def search_latency($/;"	f
search_mac	prune/search.py	/^def search_mac($/;"	f
set_input_embeddings	modeling_clip.py	/^    def set_input_embeddings(self, value):$/;"	m	class:CLIPTextModel
squad_dataloader	dataset/squad.py	/^def squad_dataloader(task_name, tokenizer, training, batch_size, max_seq_len=384, pad_to_max=False):$/;"	f
squad_dataset	dataset/squad.py	/^def squad_dataset(task_name, tokenizer, training, max_seq_len, pad_to_max=False):$/;"	f
squad_test_dataloader	dataset/squad.py	/^def squad_test_dataloader(task_name, tokenizer, batch_size, max_seq_len=384, pad_to_max=False):$/;"	f
squad_train_dataloader	dataset/squad.py	/^def squad_train_dataloader(task_name, tokenizer, batch_size, max_seq_len=384, pad_to_max=False):$/;"	f
student_inputs	prune/rescale.py	/^        student_inputs = collect_layer_inputs($/;"	v
supports_gradient_checkpointing	modeling_clip.py	/^    supports_gradient_checkpointing = True$/;"	v	class:CLIPPreTrainedModel
target_dev_metric	dataset/glue.py	/^def target_dev_metric(task_name):$/;"	f
teacher_inputs	prune/rescale.py	/^        teacher_inputs = collect_layer_inputs($/;"	v
teacher_output	prune/rescale.py	/^            teacher_output = remove_padding(teacher_output, attention_mask)$/;"	v
teacher_output	prune/rescale.py	/^            teacher_output = teacher_output[:, 0, :]$/;"	v
teacher_output	prune/rescale.py	/^        teacher_output = ffn2.fc1(hidden_states)+input_tensor # dense(hidden_states) + input_tensor$/;"	v
test_accuracy	evaluate/nlp.py	/^def test_accuracy(model, head_mask, neuron_mask, tokenizer, task_name):$/;"	f
to_dict	configuration_clip.py	/^    def to_dict(self):$/;"	m	class:CLIPConfig
to_tuple	modeling_clip.py	/^    def to_tuple(self) -> Tuple[Any]:$/;"	m	class:CLIPOutput
transpose_for_scores	generate_lut.py	/^    def transpose_for_scores(self, x):$/;"	m	class:BertMHA
update	utils/meter.py	/^     def update(self, val, n=1):$/;"	m	class:AverageMeter
vision_model	prune_clip.py	/^vision_model = model.vision_model$/;"	v
weights_per_neuron	prune/rescale.py	/^    weights_per_neuron = ffn2.fc1.weight.t()#dense.weight.t()$/;"	v
weights_per_neuron	prune/rescale.py	/^    weights_per_neuron = weights_per_neuron.index_select(dim=0, index=nonzero_neurons)$/;"	v
